{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40326a0-5d73-4041-9570-634422d73ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e7f371-5af2-4fd4-bffb-f18a8e4bc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from \n",
    "# https://github.com/google-research/google-research/blob/master/ials/vae_benchmarks/generate_data.py\n",
    "# \n",
    "# Copyright 2022 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2585ef-2aef-4775-8efd-b8b540937ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=True)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "\n",
    "\n",
    "def filter_triplets(tp, min_uc, min_sc):\n",
    "    \"\"\"Filters a DataFrame.\n",
    "    Args:\n",
    "    tp: a DataFrame of (movieId, userId, rating) triplets.\n",
    "    min_uc: filter out users with fewer than min_uc ratings.\n",
    "    min_sc: filter out items with fewer than min_sc ratings.\n",
    "    Returns:\n",
    "    A DataFrame tuple of the filtered data, the user counts and the item counts.\n",
    "    \"\"\"\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc\n",
    "    # users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but\n",
    "    # should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount\n",
    "\n",
    "\n",
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    \"\"\"Splits a DataFrame into train and test sets.\n",
    "    Args:\n",
    "    data: a DataFrame of (userId, itemId, rating).\n",
    "    test_prop: the proportion of test ratings.\n",
    "    Returns:\n",
    "    Two DataFrames of the train and test sets. The data is grouped by user, then\n",
    "    each user (with 5 ratings or more) is randomly split into train and test\n",
    "    ratings.\n",
    "    \"\"\"\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(\n",
    "                n_items_u, size=int(test_prop * n_items_u), replace=False)\n",
    "                .astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('%d users sampled' % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te\n",
    "\n",
    "\n",
    "def generate_data(raw_data, output_dir, n_heldout_users, min_uc, min_sc):\n",
    "    \"\"\"Generates and writes train, validation and test data.\n",
    "    The raw_data is first split into train, validation and test by user. For the\n",
    "    validation set, each user's ratings are randomly partitioned into two subsets\n",
    "    following a (80, 20) split (see split_train_test_proportion), and written to\n",
    "    validation_tr.csv and validation_te.csv. A similar split is applied to the\n",
    "    test set.\n",
    "    Args:\n",
    "    raw_data: a DataFrame of (userId, movieId, rating).\n",
    "    output_dir: path to the output directory.\n",
    "    n_heldout_users: this many users are held out for each of the validation and\n",
    "      test sets.\n",
    "    min_uc: filter out users with fewer than min_uc ratings.\n",
    "    min_sc: filter out items with fewer than min_sc ratings.\n",
    "    \"\"\"\n",
    "    raw_data, user_activity, item_popularity = filter_triplets(\n",
    "      raw_data, min_uc, min_sc)\n",
    "    sparsity = 1. * raw_data.shape[0] / (\n",
    "      user_activity.shape[0] * item_popularity.shape[0])\n",
    "    print('After filtering, there are %d watching events from %d users and %d '\n",
    "        'movies (sparsity: %.3f%%)' %\n",
    "        (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0],\n",
    "         sparsity * 100))\n",
    "    unique_uid = user_activity.index\n",
    "    np.random.seed(98765)\n",
    "    idx_perm = np.random.permutation(unique_uid.size)\n",
    "    unique_uid = unique_uid[idx_perm]\n",
    "    n_users = unique_uid.size\n",
    "    tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "    vd_users = unique_uid[(n_users - n_heldout_users * 2):\n",
    "                        (n_users - n_heldout_users)]\n",
    "    te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "    train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
    "    unique_sid = pd.unique(train_plays['movieId'])\n",
    "    show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "    profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "    \n",
    "    def numerize(tp):\n",
    "        uid = [profile2id[x] for x in tp['userId']]\n",
    "        sid = [show2id[x] for x in tp['movieId']]\n",
    "        return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "    pro_dir = output_dir\n",
    "    if not os.path.exists(pro_dir):\n",
    "        os.makedirs(pro_dir)\n",
    "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "        for sid in unique_sid:\n",
    "            f.write('%s\\n' % sid)\n",
    "    vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "    vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
    "    vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "    test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "    test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
    "    test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "\n",
    "    train_data = numerize(train_plays)\n",
    "    train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "    vad_data_tr = numerize(vad_plays_tr)\n",
    "    vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "    vad_data_te = numerize(vad_plays_te)\n",
    "    vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "    test_data_tr = numerize(test_plays_tr)\n",
    "    test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "    test_data_te = numerize(test_plays_te)\n",
    "    test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a79408b4-095b-457f-a61a-6d3436d8c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943ff4ea-bace-4446-9205-1c86edbec6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting Movielens 20M data\n",
      "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "Done processing Movielens 20M.\n"
     ]
    }
   ],
   "source": [
    "# MovieLens 20M\n",
    "ml20m_zip = os.path.join(output_dir, 'ml20m.zip')\n",
    "ml20m_dir = os.path.join(output_dir, 'ml-20m/')\n",
    "ml20m_file = os.path.join(output_dir, 'ml-20m/ratings.csv')\n",
    "print('Downloading and extracting Movielens 20M data')\n",
    "urllib.request.urlretrieve(\n",
    "  'http://files.grouplens.org/datasets/movielens/ml-20m.zip',\n",
    "  ml20m_zip)\n",
    "with zipfile.ZipFile(ml20m_zip, 'r') as zipref:\n",
    "    zipref.extract('ml-20m/ratings.csv', output_dir)\n",
    "os.remove(ml20m_zip)\n",
    "raw_data = pd.read_csv(ml20m_file, header=0)\n",
    "os.remove(ml20m_file)\n",
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "generate_data(\n",
    "      raw_data, output_dir=ml20m_dir, n_heldout_users=10000, min_uc=5, min_sc=0)\n",
    "print('Done processing Movielens 20M.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d4d4be3-4d43-4fb6-a95c-8ec3590af46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting Million Song Data\n",
      "After filtering, there are 33633450 watching events from 571355 users and 41140 movies (sparsity: 0.143%)\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "10000 users sampled\n",
      "11000 users sampled\n",
      "12000 users sampled\n",
      "13000 users sampled\n",
      "14000 users sampled\n",
      "15000 users sampled\n",
      "16000 users sampled\n",
      "17000 users sampled\n",
      "18000 users sampled\n",
      "19000 users sampled\n",
      "20000 users sampled\n",
      "21000 users sampled\n",
      "22000 users sampled\n",
      "23000 users sampled\n",
      "24000 users sampled\n",
      "25000 users sampled\n",
      "26000 users sampled\n",
      "27000 users sampled\n",
      "28000 users sampled\n",
      "29000 users sampled\n",
      "30000 users sampled\n",
      "31000 users sampled\n",
      "32000 users sampled\n",
      "33000 users sampled\n",
      "34000 users sampled\n",
      "35000 users sampled\n",
      "36000 users sampled\n",
      "37000 users sampled\n",
      "38000 users sampled\n",
      "39000 users sampled\n",
      "40000 users sampled\n",
      "41000 users sampled\n",
      "42000 users sampled\n",
      "43000 users sampled\n",
      "44000 users sampled\n",
      "45000 users sampled\n",
      "46000 users sampled\n",
      "47000 users sampled\n",
      "48000 users sampled\n",
      "49000 users sampled\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "10000 users sampled\n",
      "11000 users sampled\n",
      "12000 users sampled\n",
      "13000 users sampled\n",
      "14000 users sampled\n",
      "15000 users sampled\n",
      "16000 users sampled\n",
      "17000 users sampled\n",
      "18000 users sampled\n",
      "19000 users sampled\n",
      "20000 users sampled\n",
      "21000 users sampled\n",
      "22000 users sampled\n",
      "23000 users sampled\n",
      "24000 users sampled\n",
      "25000 users sampled\n",
      "26000 users sampled\n",
      "27000 users sampled\n",
      "28000 users sampled\n",
      "29000 users sampled\n",
      "30000 users sampled\n",
      "31000 users sampled\n",
      "32000 users sampled\n",
      "33000 users sampled\n",
      "34000 users sampled\n",
      "35000 users sampled\n",
      "36000 users sampled\n",
      "37000 users sampled\n",
      "38000 users sampled\n",
      "39000 users sampled\n",
      "40000 users sampled\n",
      "41000 users sampled\n",
      "42000 users sampled\n",
      "43000 users sampled\n",
      "44000 users sampled\n",
      "45000 users sampled\n",
      "46000 users sampled\n",
      "47000 users sampled\n",
      "48000 users sampled\n",
      "49000 users sampled\n",
      "Done processing Million Song Data.\n"
     ]
    }
   ],
   "source": [
    "# MSD Dataset\n",
    "print('Downloading and extracting Million Song Data')\n",
    "msd_zip = os.path.join(output_dir, 'msd.zip')\n",
    "msd_dir = os.path.join(output_dir, 'msd/')\n",
    "msd_file = os.path.join(output_dir, 'msd/train_triplets.txt')\n",
    "urllib.request.urlretrieve(\n",
    "  'http://millionsongdataset.com/sites/default/files/challenge/train_triplets.txt.zip',\n",
    "  msd_zip)\n",
    "with zipfile.ZipFile(msd_zip, 'r') as zipref:\n",
    "    zipref.extractall(msd_dir)\n",
    "    \n",
    "os.remove(msd_zip)\n",
    "raw_data = pd.read_csv(\n",
    "  msd_file, sep='\\t', header=None, names=['userId', 'movieId', 'count'])\n",
    "os.remove(msd_file)\n",
    "generate_data(\n",
    "  raw_data, output_dir=msd_dir, n_heldout_users=50000, min_uc=20,\n",
    "  min_sc=200)\n",
    "print('Done processing Million Song Data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519dbd10-1412-4912-b455-941f093d6401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99389fe2-397f-4287-b1c3-fb772ce6a704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fa3015e-e2f3-4de7-b7a2-b5c139cfce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "872bc8e2-f1a7-4722-a2e1-361199c3a93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting Million Song Data\n"
     ]
    }
   ],
   "source": [
    "print('Downloading and extracting Million Song Data')\n",
    "msd_zip = os.path.join(output_dir, 'msd.zip')\n",
    "msd_dir = os.path.join(output_dir, 'msd/')\n",
    "msd_file = os.path.join(output_dir, 'msd/train_triplets.txt')\n",
    "urllib.request.urlretrieve(\n",
    "  'http://millionsongdataset.com/sites/default/files/challenge/train_triplets.txt.zip',\n",
    "  msd_zip)\n",
    "with zipfile.ZipFile(msd_zip, 'r') as zipref:\n",
    "    zipref.extractall(msd_dir)\n",
    "    \n",
    "os.remove(msd_zip)\n",
    "raw_data = pd.read_csv(\n",
    "  msd_file, sep='\\t', header=None, names=['userId', 'movieId', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acd741a4-0969-411e-a5f4-4c6ac34b0eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384546,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.movieId.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785cb4c-f50b-4660-ae2c-244244a2d679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a945f-c0d0-4b92-8280-fe145ed96c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6407d884-0022-431c-b180-eb00f64009c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to netflix data\n",
    "# Netflix-Prize data can be downloaded from Kaggle:\n",
    "# https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data\n",
    "# The data can NOT be downloaded from http://www.netflixprize.com/ anymore.\n",
    "path_netflix = '/home/pmoritz/Downloads/archive.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "142a0576-d83a-4e60-9a93-2c788579ffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Netflix Data\n",
      "After filtering, there are 56880037 watching events from 463435 users and 17769 movies (sparsity: 0.691%)\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "10000 users sampled\n",
      "11000 users sampled\n",
      "12000 users sampled\n",
      "13000 users sampled\n",
      "14000 users sampled\n",
      "15000 users sampled\n",
      "16000 users sampled\n",
      "17000 users sampled\n",
      "18000 users sampled\n",
      "19000 users sampled\n",
      "20000 users sampled\n",
      "21000 users sampled\n",
      "22000 users sampled\n",
      "23000 users sampled\n",
      "24000 users sampled\n",
      "25000 users sampled\n",
      "26000 users sampled\n",
      "27000 users sampled\n",
      "28000 users sampled\n",
      "29000 users sampled\n",
      "30000 users sampled\n",
      "31000 users sampled\n",
      "32000 users sampled\n",
      "33000 users sampled\n",
      "34000 users sampled\n",
      "35000 users sampled\n",
      "36000 users sampled\n",
      "37000 users sampled\n",
      "38000 users sampled\n",
      "39000 users sampled\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "10000 users sampled\n",
      "11000 users sampled\n",
      "12000 users sampled\n",
      "13000 users sampled\n",
      "14000 users sampled\n",
      "15000 users sampled\n",
      "16000 users sampled\n",
      "17000 users sampled\n",
      "18000 users sampled\n",
      "19000 users sampled\n",
      "20000 users sampled\n",
      "21000 users sampled\n",
      "22000 users sampled\n",
      "23000 users sampled\n",
      "24000 users sampled\n",
      "25000 users sampled\n",
      "26000 users sampled\n",
      "27000 users sampled\n",
      "28000 users sampled\n",
      "29000 users sampled\n",
      "30000 users sampled\n",
      "31000 users sampled\n",
      "32000 users sampled\n",
      "33000 users sampled\n",
      "34000 users sampled\n",
      "35000 users sampled\n",
      "36000 users sampled\n",
      "37000 users sampled\n",
      "38000 users sampled\n",
      "39000 users sampled\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Netflix Data')\n",
    "netflix_zip = os.path.join(output_dir, 'netflix.zip')\n",
    "netflix_dir = os.path.join(output_dir, 'netflix/')\n",
    "netflix_files = [os.path.join(output_dir, f'netflix/combined_data_{i}.txt') for i in range(1,5)]\n",
    "shutil.copyfile(path_netflix, netflix_zip)\n",
    "with zipfile.ZipFile(netflix_zip, 'r') as zipref:\n",
    "    zipref.extractall(netflix_dir)\n",
    "    \n",
    "os.remove(netflix_zip)\n",
    "raw_data = pd.DataFrame(columns=['movie_id', 'user_id', 'rating'])\n",
    "\n",
    "# for file in files:\n",
    "for file in netflix_files:\n",
    "    movie_ids = []\n",
    "    user_ids = []\n",
    "    ratings = []\n",
    "    with open(file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            if line[-2] == ':':\n",
    "                movie_id = line.split(':')[0]\n",
    "            else:\n",
    "                user_id, rating, _ = line.split(',')\n",
    "                movie_ids.append(movie_id)\n",
    "                user_ids.append(user_id)\n",
    "                ratings.append(rating)\n",
    "                \n",
    "    raw_data_part = pd.DataFrame({'movie_id': np.array(movie_ids).astype(int), \n",
    "                                 'user_id': np.array(user_ids).astype(int), \n",
    "                                 'rating': np.array(ratings).astype(float)})\n",
    "    raw_data = pd.concat((raw_data, raw_data_part), ignore_index=True)\n",
    "    \n",
    "movie_ids = None\n",
    "user_ids = None\n",
    "ratings = None\n",
    "raw_data = raw_data.rename(columns={'movie_id': 'movieId', 'user_id': 'userId'})\n",
    "raw_data.sort_values(by=['userId', 'movieId'], inplace=True)\n",
    "raw_data.reset_index(inplace=True)\n",
    "raw_data = raw_data[raw_data.rating > 3.5]\n",
    "generate_data(raw_data, output_dir=netflix_dir, n_heldout_users=40000, min_uc=5, min_sc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e738f8-1c39-4209-af79-96a918135b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc1af4-2d19-4127-b169-3a965640d9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0edf051a-a01c-49e6-a7b6-4605ca641c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_netflix = '/home/pmoritz/Downloads/archive.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe02f539-d859-4cf3-aff0-ea28ace545eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Netflix Data\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Netflix Data')\n",
    "netflix_zip = os.path.join(output_dir, 'netflix.zip')\n",
    "netflix_dir = os.path.join(output_dir, 'netflix/')\n",
    "netflix_files = [os.path.join(output_dir, f'netflix/combined_data_{i}.txt') for i in range(1,5)]\n",
    "shutil.copyfile(path_netflix, netflix_zip)\n",
    "with zipfile.ZipFile(netflix_zip, 'r') as zipref:\n",
    "    zipref.extractall(netflix_dir)\n",
    "    \n",
    "os.remove(netflix_zip)\n",
    "raw_data = pd.DataFrame(columns=['movie_id', 'user_id', 'rating'])\n",
    "\n",
    "# for file in files:\n",
    "for file in netflix_files:\n",
    "    movie_ids = []\n",
    "    user_ids = []\n",
    "    ratings = []\n",
    "    with open(file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            if line[-2] == ':':\n",
    "                movie_id = line.split(':')[0]\n",
    "            else:\n",
    "                user_id, rating, _ = line.split(',')\n",
    "                movie_ids.append(movie_id)\n",
    "                user_ids.append(user_id)\n",
    "                ratings.append(rating)\n",
    "                \n",
    "    raw_data_part = pd.DataFrame({'movie_id': np.array(movie_ids).astype(int), \n",
    "                                 'user_id': np.array(user_ids).astype(int), \n",
    "                                 'rating': np.array(ratings).astype(float)})\n",
    "    raw_data = pd.concat((raw_data, raw_data_part), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fde7fed7-f8b5-4d25-b693-3158a9f49237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17770,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.movie_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119b8c6-58bc-422b-a3ab-0a11321b1835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
